from __future__ import annotations

from typing import Annotated, Any, Dict, List, Optional

from langchain_core.messages import AIMessage, SystemMessage
from langgraph.graph.message import add_messages
from langgraph.runtime import Runtime
from pydantic import BaseModel, Field
from typing_extensions import TypedDict

from src.llm import model, model_with_tools
from src.logger import logger


class Context(TypedDict):
    """Context parameters for the agent.

    Set these when creating assistants OR when invoking the graph.
    See: https://langchain-ai.github.io/langgraph/cloud/how-tos/configuration_cloud/
    """

    my_configurable_param: str


# --- å®šç¾© LLM è¼¸å‡ºçš„çµæ§‹ ---
class ProblemExtraction(BaseModel):
    job_title: Optional[str] = Field(description="ç”¨æˆ¶çš„è·ä½ï¼Œè‹¥ç„¡å‰‡ç•™ç©º")
    pain_point: Optional[str] = Field(description="ç”¨æˆ¶æåˆ°çš„å•é¡Œç—›é»ï¼Œè‹¥ç„¡å‰‡ç•™ç©º")
    goal: Optional[str] = Field(description="ç”¨æˆ¶æƒ³é”æˆçš„ç›®æ¨™ï¼Œè‹¥ç„¡å‰‡ç•™ç©º")


class EvaluationDimensions(BaseModel):
    pain_point_score: int = Field(..., description="ç—›é»æè¿°çš„å…·é«”ç¨‹åº¦ (0-30)")
    goal_metric_score: int = Field(..., description="ç›®æ¨™èˆ‡æŒ‡æ¨™çš„æ¸…æ™°åº¦ (0-40)")
    box_trap_score: int = Field(..., description="æ˜¯å¦è·³è„«ã€æ‰‹æ®µç•¶ç›®çš„ã€çš„é™·é˜± (0-30)")


class ProblemEvaluation(BaseModel):
    score: int = Field(..., description="ç¸½åˆ† (0-100)")
    dimensions: EvaluationDimensions
    is_passing: bool = Field(..., description="æ˜¯å¦é€šéé–€æª»")
    critique: str = Field(..., description="çŠ€åˆ©çš„è©•èª")
    advice: str = Field(..., description="çµ¦ç”¨æˆ¶çš„å¼•å°å»ºè­°")
    missing_fields: list[str] = Field(..., description="ç¼ºå°‘çš„é—œéµè³‡è¨Šæ¬„ä½")


class CrossSiloOutput(BaseModel):
    result: Optional[str] = Field(..., description="è·¨éƒ¨é–€è¦–è§’çš„è¦‹è§£")
    score: int = Field(..., description="ç­–ç•¥å®Œæ•´åº¦åˆ†æ•¸ (0-100)")


class State(BaseModel):
    """Input state for the agent.

    Defines the initial structure of incoming data.
    See: https://langchain-ai.github.io/langgraph/concepts/low_level/#state
    """

    messages: Annotated[List[Any], add_messages]
    # ç”¨ä¾†è¿½è¹¤ä¸‰å€‹é—œéµè³‡è¨Šè’é›†ç‹€æ³
    problem_profile: dict = Field(
        default_factory=lambda: {
            "pain_point": None,  # ç—›é»
            "goal": None,  # ç›®æ¨™
        }
    )

    # è©•ä¼°çµæœ
    reflection_result: dict = Field(
        default_factory=lambda: {
            "is_complete": False,
            "missing_fields": [],  # ä¾‹å¦‚ ["metric", "goal"]
        }
    )
    evaluation_result: dict = Field(
        default_factory=lambda: {
            "score": 0,
            "critique": "",
            "advice": "",
            "missing_fields": [],
        }
    )
    cross_silo_output: dict = Field(
        default_factory=lambda: {
            "result": "",
            "score": 0,
        }
    )
    job_title: Optional[str] = None
    is_passing_evaluation: bool = False
    node_status: str = "example"
    last_stage: str = ""  # è¿½è¹¤ä¸Šä¸€è¼ªçš„éšæ®µ: "reflection", "refine_ask",

    count_node_file_export: int = 0


async def node_situation(state: State, runtime: Runtime[Context]) -> Dict[str, Any]:
    logger.info("=== é€²å…¥ node_situation ===")
    current_profile = state.problem_profile

    # 1. å‘¼å« LLM é€²è¡Œæå–
    extraction_prompt = f"""
    ç›®å‰å·²çŸ¥çš„è³‡è¨Š: {current_profile}
    ä½ æ˜¯ä¸€å€‹ç­–ç•¥é¡§å•ï¼Œå°ˆé–€å”åŠ©ä¼æ¥­é«˜å±¤é‡æ¸…ä»–çš„è·ä½èˆ‡å°ˆæ¡ˆç›®æ¨™, 
    è«‹åˆ†æä¸»ç®¡çš„æœ€æ–°å›ç­”ï¼Œæå–æˆ–æ›´æ–° 'job_title', 'pain_point', 'goal'ã€‚
    
    è¦å‰‡ï¼š
    1. å¦‚æœä¸»ç®¡çš„å›ç­”ä¸­åŒ…å«äº†æ–°çš„è³‡è¨Šï¼Œè«‹æå–ä¸¦å›å‚³ã€‚
    2. å¦‚æœä¸»ç®¡çš„å›ç­”ä¸­æ²’æœ‰æåˆ°æŸé …è³‡è¨Šï¼Œè«‹å›å‚³ Noneã€‚
    3. åªæœ‰åœ¨ä¸»ç®¡æ˜ç¢ºæƒ³è¦ä¿®æ”¹æˆ–è£œå……æ™‚æ‰æ›´æ–°ã€‚
    """

    # ä½¿ç”¨ with_structured_output ç¶å®š schema
    structured_model = model.with_structured_output(ProblemExtraction)
    messages = state.messages[-1]
    messages_to_send = [SystemMessage(content=extraction_prompt), messages]

    extracted_data: ProblemExtraction = await structured_model.ainvoke(messages_to_send)
    logger.info(f"extracted data: {extracted_data}")

    # 2. åˆä½µè³‡æ–™ (Merge)
    # å› ç‚º LLM å¯èƒ½åªå›å‚³é€™æ¬¡æå–åˆ°çš„ï¼Œæˆ‘å€‘è¦è·ŸèˆŠçš„ state åˆä½µ
    new_profile = current_profile.copy()

    # åªæœ‰åœ¨æå–åˆ°æœ‰æ•ˆå…§å®¹æ™‚æ‰ç´¯åŠ ï¼Œé¿å… None è¦†è“‹å·²æœ‰è³‡è¨Š
    if extracted_data.pain_point is not None:
        if new_profile["pain_point"]:
            # ç´¯åŠ æ–°è³‡è¨Š
            new_profile["pain_point"] += " " + extracted_data.pain_point
        else:
            new_profile["pain_point"] = extracted_data.pain_point

    if extracted_data.goal is not None:
        if new_profile["goal"]:
            # ç´¯åŠ æ–°è³‡è¨Š
            new_profile["goal"] += " " + extracted_data.goal
        else:
            new_profile["goal"] = extracted_data.goal

    logger.info(f"new profile: {new_profile}")

    # 3. æª¢æŸ¥ç¼ºå¤± (Check Missing)
    missing_fields = [k for k, v in new_profile.items() if not v]
    is_complete = len(missing_fields) == 0

    advice = ""
    if not is_complete:
        advice = f"ç›®å‰é‚„ç¼ºå°‘ä»¥ä¸‹è³‡è¨Šï¼š{', '.join(missing_fields)}ã€‚è«‹è¿½å•ç”¨æˆ¶ã€‚"
    logger.info(f"missing fields: {missing_fields}")
    logger.info(f"is complete: {is_complete}")

    if state.job_title is None and extracted_data.job_title is not None:
        job_title = extracted_data.job_title
    else:
        job_title = state.job_title

    return {
        "problem_profile": new_profile,
        "reflection_result": {
            "is_complete": is_complete,
            "missing_fields": missing_fields,
            "advice": advice,
        },
        "job_title": job_title,
        "node_status": f"output from situation. Configured with {(runtime.context or {}).get('my_configurable_param')}",
        "last_stage": "situation",
    }

async def node_reflection(state: State, runtime: Runtime[Context]) -> Dict[str, Any]:
    logger.info("=== é€²å…¥ node_reflection ===")
    profile = state.problem_profile
    missing = state.reflection_result["missing_fields"]
    logger.info(f"reflection: missing fields{missing}")
    logger.info(f"problem profile: {profile}")
    system_prompt = f"""
    ä½ æ˜¯ä¸€å€‹ç­–ç•¥é¡§å•ï¼Œå°ˆé–€å”åŠ©ä¼æ¥­é«˜å±¤é‡æ¸…å°ˆæ¡ˆç›®æ¨™ã€‚
    ç›®å‰å·²çŸ¥è³‡è¨Šå¦‚ä¸‹ï¼š
    - ç—›é»: {profile["pain_point"]}
    - ç›®æ¨™: {profile["goal"]}
    - ç¼ºå¤±è³‡è¨Š: {missing}   
    ä½ çš„ä»»å‹™æ˜¯è¨­è¨ˆä¸€å€‹é‡å°æ€§çš„å•é¡Œï¼Œå¼•å°ä¸»ç®¡è£œå……ç¼ºå¤±è³‡è¨Š"{missing}"ã€‚
    æ³¨æ„ï¼š
    - ä¸è¦æ‰“æ‹›å‘¼ä¹Ÿä¸è¦çµ¦æ¨™é¡Œï¼Œç›´æ¥å•å•é¡Œ
    """

    last_message = state.messages[-1]

    messages_to_send = [SystemMessage(content=system_prompt), last_message]
    msg = await model.ainvoke(messages_to_send)

    new_profile = profile.copy()
    if profile["pain_point"] is not None:
        new_profile["pain_point"] += msg.content if "pain_point" in missing else ""
    if profile["goal"] is not None:
        new_profile["goal"] += msg.content if "goal" in missing else ""

    return {
        "node_status": f"output from reflection. Configured with {(runtime.context or {}).get('my_configurable_param')}",
        "messages": [msg],
        "last_stage": "reflection",
        "problem_profile": new_profile,
    }


async def node_summary(state: State, runtime: Runtime[Context]) -> Dict[str, Any]:
    logger.info("=== é€²å…¥ node_summary ===")
    profile = state.problem_profile
    logger.info({"before summary:": profile})
    prompt = f"""
    ä½ æ˜¯ä¸€ä½ç­–ç•¥é¡§å•ï¼Œè«‹é‡å°ç—›é»èˆ‡ç›®æ¨™çš„å•ç­”æ–‡æœ¬ï¼Œåˆ†åˆ¥é‡å°ç—›é»å’Œç›®æ¨™åšæ‘˜è¦, éœ€è¦è¨˜ä½ä¸»ç®¡æ¯ä¸€å€‹æåˆ°çš„é», ä¸å¯ä»¥æ¼æ‰ä»»ä½•è³‡è¨Šã€‚
    ç—›é»: {profile["pain_point"]}
    ç›®æ¨™: {profile["goal"]}

    æ³¨æ„ï¼š
    - åªéœ€å›è¦†ç²¾ç°¡æ‘˜è¦ï¼Œç„¡éœ€å¤šé¤˜çš„èªªæ˜æˆ–æ‰“æ‹›å‘¼
    """

    structured_model = model.with_structured_output(ProblemExtraction)
    last_message = state.messages[-1]
    msg = await structured_model.ainvoke([SystemMessage(content=prompt), last_message])

    new_proflile = {
        "pain_point": msg.pain_point,
        "goal": msg.goal,
    }
    logger.info(f"after summary: {new_proflile}")
    return {
        "node_status": "Summary generated.",
        "problem_profile": new_proflile,
        "last_stage": "summary",
    }


async def node_evaluation(state: State, runtime: Runtime[Context]) -> Dict[str, Any]:
    """å¦‚æœè³‡è¨Šéƒ½é½Šå…¨äº†ï¼Œå°±è©•ä¼°å•é¡Œæè¿°çš„å“è³ªï¼Œä¸¦çµ¦åˆ†æ•¸å’Œå»ºè­°."""
    logger.info("=== é€²å…¥ node_evaluation ===")
    profile = state.problem_profile
    prompt = f"""
    # Role
    ä½ æ˜¯ä¸€ä½å°ˆç²¾æ–¼ã€Œç ´æ¡†æ€ç¶­ (Break-the-Box Thinking)ã€çš„å°ç£ç­–ç•¥é¡§å•ã€‚ä½ çš„ä»»å‹™ä¸æ˜¯å–®ç´”çš„èŠå¤©ï¼Œè€Œæ˜¯ç”¨è¦ªåˆ‡ä¸”ä¸­è‚¯çš„èªæ°£è©•ä¼°ç”¨æˆ¶æå‡ºçš„ã€Œå•é¡Œé™³è¿°ã€æ˜¯å¦å…·å‚™æˆ°ç•¥è§£æ±ºçš„åƒ¹å€¼ã€‚

    # Task
    è«‹åˆ†æç”¨æˆ¶è¼¸å…¥çš„æ–‡å­—ï¼Œæ ¹æ“šä»¥ä¸‹å››å€‹ç¶­åº¦é€²è¡Œåš´æ ¼è©•åˆ† (0-100)ï¼Œä¸¦çµ¦å‡ºä¸­è‚¯çš„è©•èªã€‚
    ç—›é»ï¼š{profile["pain_point"]}
    ç›®æ¨™ï¼š{profile["goal"]}

    # Evaluation Criteria (Rubric)

    1. **Pain Point (30åˆ†)**
    - 0-10åˆ†: åªèªªäº†æ„Ÿè¦º (e.g., "å¾ˆç´¯", "å¾ˆé›£")ã€‚
    - 11-20åˆ†: æåˆ°äº†å¤§è‡´ç‹€æ³ï¼Œä½†ç¼ºä¹æƒ…å¢ƒã€‚
    - 21-30åˆ†: æ¸…æ¥šæè¿°äº† "èª°" åœ¨ "ä»€éº¼æƒ…å¢ƒ" ä¸‹é‡åˆ°äº† "ä»€éº¼å…·é«”é˜»ç¤™"ã€‚

    2. **Goal & Metric (40åˆ†)**
    - 0-10åˆ†: å®Œå…¨æ²’æåˆ°ç›®æ¨™æˆ–æ•¸å­—ã€‚
    - 11-25åˆ†: æœ‰ç›®æ¨™ä½†ç„¡é‡åŒ–æŒ‡æ¨™ (e.g., "æƒ³æå‡æ•ˆç‡")ã€‚
    - 26-40åˆ†: æœ‰æ˜ç¢ºçš„æˆåŠŸå®šç¾©èˆ‡é‡åŒ–æŒ‡æ¨™ (e.g., "æå‡ 20% è½‰æ›ç‡")ã€‚

    3. **Solution Bias (Box Trap) (30åˆ†)**
    - è­¦å‘Šï¼šé€™æ˜¯ç ´æ¡†æ€ç¶­çš„æ ¸å¿ƒã€‚
    - 0åˆ† (é™·å…¥æ¡†æ¡†): ç”¨æˆ¶ç›´æ¥æŠŠ "è§£æ±ºæ–¹æ¡ˆ" ç•¶æˆå•é¡Œ (e.g., "æˆ‘éœ€è¦å°å…¥ AI", "æˆ‘éœ€è¦åšä¸€å€‹ App")ã€‚é€™ä¸æ˜¯å•é¡Œï¼Œé€™æ˜¯æ‰‹æ®µã€‚
    - 30åˆ† (ç ´æ¡†): ç”¨æˆ¶å°ˆæ³¨æ–¼ "æƒ³è§£æ±ºçš„æœ¬è³ªå›°é›£" æˆ– "æƒ³å‰µé€ çš„åƒ¹å€¼"ï¼Œè€Œéé™å®šæŸç¨®å·¥å…·ã€‚

        """
    structured_model = model.with_structured_output(ProblemEvaluation)
    last_message = state.messages[-1]
    messages_to_send = [SystemMessage(content=prompt), last_message]
    response = await structured_model.ainvoke(messages_to_send)

    # Format the evaluation result as a readable message
    evaluation_message = f"""
        ğŸ“Š **è©•åˆ†çµæœ** (ç¸½åˆ†: {response.score}/100)

        **è©•åˆ†ç´°é …:**
        - ç—›é»æè¿°: {response.dimensions.pain_point_score}/30
        - ç›®æ¨™èˆ‡æŒ‡æ¨™: {response.dimensions.goal_metric_score}/40
        - ç ´æ¡†æ€ç¶­: {response.dimensions.box_trap_score}/30

        **è©•èª:**
        {response.critique}

        **å»ºè­°:**
        {response.advice}
        """

    if response.score >= 65:
        response.is_passing = True

    eval_result = {
        "score": response.score,
        "critique": response.critique,
        "advice": response.advice,
        "missing_fields": response.missing_fields,
    }
    logger.info(f"Returning evaluation_result: {eval_result}")
    logger.info(f"is_passing_evaluation: {response.is_passing}")

    return {
        "node_status": f"output from evaluation. Configured with {(runtime.context or {}).get('my_configurable_param')}",
        # "messages": [SystemMessage(content=evaluation_message)],
        "is_passing_evaluation": response.is_passing,
        "evaluation_result": eval_result,
        "last_stage": "evaluation",
    }


async def node_refine_ask(state: State):
    logger.info("=== é€²å…¥ node_refine_ask ===")
    result = state.evaluation_result
    advice = result["advice"]
    critique = result["critique"]
    missing = result["missing_fields"]
    logger.info(f"refine_ask result: {result}")

    prompt = f"""
    å‰›æ‰çš„è©•ä¼°çµæœé¡¯ç¤ºé«˜å±¤å®šç¾©å•é¡Œå¯ä»¥æ›´å¥½ã€‚
    è©•èªï¼š{critique}
    å»ºè­°æ–¹å‘ï¼š{advice}
    ç¼ºå¤±è³‡è¨Šï¼š{missing}

    è«‹æ ¹æ“šä¸Šè¿°å»ºè­°ï¼Œæ‰®æ¼”è¦ªåˆ‡ä½†å°ˆæ¥­çš„é¡§å•ï¼Œç”¨200å­—ä»¥å…§çš„å•é¡Œå¼•å°é«˜å±¤æ·±å…¥æŒ–æ˜è­°é¡Œä»¥è£œè¶³ç¼ºå¤±è³‡è¨Šã€‚
    æ³¨æ„ï¼š
    - ä¸è¦æ‰“æ‹›å‘¼ä¹Ÿä¸è¦çµ¦æ¨™é¡Œï¼Œç›´æ¥å•å•é¡Œ
    - å•é¡Œè¦ç²¾ç°¡ï¼Œä¸”é™„ä¸Šä¸€å€‹èˆ‰ä¾‹å¹«åŠ©ç†è§£
    """
    logger.info(f"refine_ask prompt: {prompt}")
    last_message = state.messages[-1]
    msg = await model.ainvoke([SystemMessage(content=prompt), last_message])

    return {"messages": [msg], "last_stage": "refine_ask"}


async def node_hmw_gen(state: State):
    """å°‡å•é¡Œæ”¹ç‚ºå¦‚ä½•...é–‹é ­."""
    logger.info("=== é€²å…¥ node_hmw_gen ===")
    prompt = f"""
    ä½ æ˜¯ä¸€ä½ç­–ç•¥é¡§å•ï¼Œè«‹å°‡ä»¥ä¸‹å•é¡Œé™³è¿°æ”¹å¯«æˆä»¥ä¸‹çš„å›è¦†æ ¼å¼ï¼Œå¹«åŠ©ç”¨æˆ¶èšç„¦åœ¨è§£æ±ºæ–¹æ¡ˆçš„æ¢ç´¢ä¸Šã€‚
    å•é¡Œé™³è¿°: {state.problem_profile}
    å›è¦†æ ¼å¼ç¯„ä¾‹: ç¸½çµæ‚¨æƒ³è§£æ±ºçš„å•é¡Œï¼šåœ¨...çš„æƒ…å¢ƒä¸‹ï¼Œå¦‚ä½•...ï¼Ÿ
    æ³¨æ„ï¼š
    - ç„¡éœ€å¤šé¤˜çš„èªªæ˜æˆ–æ‰“æ‹›å‘¼
    
    """
    last_message = state.messages[-1]
    msg = await model.ainvoke([SystemMessage(content=prompt), last_message])
    return {
        "messages": [msg],
        "node_status": "HMW question generated.",
        "last_stage": "hmw_gen",
    }


async def node_cross_silo(state: State):
    """å¾è·¨éƒ¨é–€è¦–è§’å¯©è¦–ç—›é»å’Œç›®æ¨™"""
    logger.info("=== é€²å…¥ node_cross_silo ===")

    current_result = state.cross_silo_output.get("result", "")

    # å¦‚æœ result ç‚ºç©ºï¼Œä»£è¡¨æ˜¯ç¬¬ä¸€æ¬¡é€²å…¥æ­¤ç¯€é»ï¼Œé€²è¡Œæå•
    if not current_result:
        prompt = f"""
        ä½ æ˜¯ä¸€ä½è·¨é ˜åŸŸçš„ç­–ç•¥é¡§å•ï¼Œå°ˆé–€å”åŠ©é«˜å±¤å¾è·¨éƒ¨é–€çš„è§’åº¦å¯©è¦–å•é¡Œæ‰€éœ€è¦çš„è³‡æºã€‚
        è©¢å•ä¸»ç®¡å®Œæ•´å¥å­ï¼šâ€œå¾{state.job_title}è·ä½ä¾†çœ‹ï¼Œé€™å€‹ç—›é»å°æ‚¨æœ‰ä»€éº¼å½±éŸ¿ï¼Ÿæ‚¨éœ€è¦å…¶ä»–éƒ¨é–€æä¾›å“ªäº›è³‡æºæˆ–èƒ½åŠ›ä¾†å”åŠ©è§£æ±ºï¼Ÿ
        ä¸¦å¾ä¸»ç®¡è·ç‚ºä»¥åŠå…ˆå‰çµ¦çš„ç—›é»å’Œç›®æ¨™èˆ‰å€‹ä¾‹å­å¼•å°ä»–æ€è€ƒ: 
        è·ä½ï¼š{state.job_title}
        ç—›é»èˆ‡ç›®æ¨™ï¼š{state.problem_profile}
        æ³¨æ„ï¼š
        - åªéœ€å›è¦†ï¼Œç„¡éœ€å¤šé¤˜çš„èªªæ˜æˆ–æ‰“æ‹›å‘¼
        - ä¾‹å­è¦å¾é«˜å±¤çš„è·ä½å‡ºç™¼ï¼Œä¸¦ä¸”å…·é«”èªªæ˜éƒ¨é–€å¯èƒ½éœ€è¦çš„è³‡æº
        """
        msg = await model.ainvoke([SystemMessage(content=prompt)])
        updated_result = f"AI Question: {msg.content}"

        return {
            "messages": [msg],
            "cross_silo_output": {
                "result": updated_result,
                "score": 0,
            },
            "node_status": "Asking cross-silo resources.",
            "last_stage": "cross_silo",
        }

    else:
        # éç¬¬ä¸€æ¬¡é€²å…¥ï¼Œä»£è¡¨ç”¨æˆ¶å·²å›è¦†ï¼Œé€²è¡Œè©•ä¼°
        last_message = state.messages[-1]
        updated_result = current_result + f"\nUser Answer: {last_message.content}"

        prompt = f"""
        ä½ æ˜¯ä¸€ä½è·¨é ˜åŸŸçš„ç­–ç•¥é¡§å•ï¼Œå°ˆé–€å”åŠ©é«˜å±¤å¾è·¨éƒ¨é–€çš„è§’åº¦å¯©è¦–å•é¡Œæ‰€éœ€è¦çš„è³‡æºã€‚
        æ ¹æ“šå…ˆå‰çš„è¨è«–ï¼Œç¹¼çºŒå›ç­”å•é¡Œæˆ–æ˜¯å•ä¸€å€‹å•é¡Œå¼•å°ä¸»ç®¡æ·±å…¥æ€è€ƒã€‚
        åŒæ™‚ï¼Œçµ¦äºˆè¨è«–å®Œæ•´æ€§æ‰“ä¸€å€‹åˆ†æ•¸ (0-100)ï¼Œä¸¦çµ¦äºˆå»ºè­°å’Œç†ç”±ã€‚
        è·ä½ï¼š{state.job_title}
        ç—›é»èˆ‡ç›®æ¨™ï¼š{state.problem_profile}
        å…ˆå‰è¨è«–ï¼š{updated_result}
        
        è©•åˆ†æ¨™æº–ï¼š
        - < 65åˆ†ï¼šå›ç­”æ¨¡ç³Šæˆ–ç¼ºä¹å…·é«”è·¨éƒ¨é–€è³‡æºéœ€æ±‚ -> ç¹¼çºŒè¿½å•
        - >= 65åˆ†ï¼šå›ç­”å…·é«”ä¸”å®Œæ•´ -> ä¸å›æ‡‰
        
        æ³¨æ„ï¼š
        - åªéœ€å›è¦†ï¼Œç„¡éœ€å¤šé¤˜çš„èªªæ˜æˆ–æ‰“æ‹›å‘¼
        - ä¾‹å­è¦å¾é«˜å±¤çš„è·ä½å‡ºç™¼ï¼Œä¸¦ä¸”å…·é«”èªªæ˜éƒ¨é–€å¯èƒ½éœ€è¦çš„è³‡æº
        """

        structured_model = model.with_structured_output(ProblemEvaluation)
        eval_result = await structured_model.ainvoke(
            [SystemMessage(content=prompt), last_message]
        )

        # ç”±æ–¼ ProblemEvaluation æ²’æœ‰ content æ¬„ä½ï¼Œæˆ‘å€‘ä½¿ç”¨ advice ä½œç‚ºå›æ‡‰
        response_content = eval_result.advice
        updated_result += f"\nAI Advice: {response_content}"

        logger.info(f"Cross-silo score: {eval_result.score}")

        return {
            "messages": [AIMessage(content=response_content)],
            "cross_silo_output": {
                "result": updated_result,
                "score": eval_result.score,
            },
            "node_status": "Cross-silo perspectives evaluated.",
            "last_stage": "cross_silo",
        }


async def node_final_summary(state: State):
    """ç”¢ç”Ÿæœ€çµ‚çš„å•é¡Œæè¿°ç¸½çµ."""
    logger.info("=== é€²å…¥ node_final_summary ===")

    profile = state.problem_profile
    prompt = f"""
    ä½ æ˜¯ä¸€ä½ç­–ç•¥é¡§å•ï¼Œè«‹æ ¹æ“šä»¥ä¸‹è³‡è¨Šï¼Œç”¢ç”Ÿä¸€å€‹å®Œæ•´ä¸”å…·é«”çš„ç­–ç•¥å ±å‘Šï¼Œå¹«åŠ©ç”¨æˆ¶èšç„¦åœ¨æ ¸å¿ƒè­°é¡Œä¸Šã€‚
    ç—›é»: {profile["pain_point"]}
    ç›®æ¨™: {profile["goal"]}
    è·¨éƒ¨é–€è¦–è§’: {state.cross_silo_output["result"]}        
    æ³¨æ„ï¼š
    - ç­–ç•¥è¦å…·é«”ä¸”å…·å‚™å¯è¡Œæ€§ï¼Œç„¡éœ€å¤šé¤˜çš„èªªæ˜æˆ–æ‰“æ‹›å‘¼
    """
    msg = await model.ainvoke([SystemMessage(content=prompt)])
    return {
        "messages": [msg],
        "node_status": "Strategy summary generated.",
        "last_stage": "final_summary",
    }


async def node_file_export(state: State):
    """å°‡å ±å‘Šè¼¸å‡ºç‚ºppt"""
    logger.info("=== é€²å…¥ node_file_export ===")

    prompt = """
    ä½ æ˜¯ä¸€ä½è²¼å¿ƒçš„åŠ©ç†ã€‚
    è«‹æ ¹æ“šç”¨æˆ¶çš„å›è¦†æ±ºå®šä¸‹ä¸€æ­¥è¡Œå‹•ï¼š
    1. è‹¥ç”¨æˆ¶åŒæ„è£½ä½œ PPT ç°¡å ±ï¼Œè«‹æ ¹æ“šå°è©±æ­·å²ä¸­çš„ã€ç­–ç•¥å ±å‘Šã€å…§å®¹ï¼Œä½¿ç”¨ generate_ppt å·¥å…·ä¾†ç”Ÿæˆæª”æ¡ˆã€‚
       - filename è«‹ä½¿ç”¨è‹±æ–‡ (ä¾‹å¦‚ strategy_report)
       - title è«‹ä½¿ç”¨å ±å‘Šçš„æ¨™é¡Œ
       - bullet_points è«‹å°‡å ±å‘Šä¸­çš„é—œéµç­–ç•¥æ•´ç†æˆåˆ—é»
    2. è‹¥ç”¨æˆ¶ä¸éœ€è¦æˆ–æ‹’çµ•ï¼Œè«‹ç¦®è²Œå›æ‡‰ä¸¦çµæŸå°è©±ã€‚
    """

    msg = await model_with_tools.ainvoke(
        [SystemMessage(content=prompt)] + state.messages
    )
    logger.info(f"File export: {msg.content}, Tool calls: {msg.tool_calls}")
    return {
        "messages": [msg],
        "node_status": "Exporting file",
        "last_stage": "file_export",
    }
